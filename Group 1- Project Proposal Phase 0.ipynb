{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## Assessing Risk For Home Credit Applications \n",
    "###### Naimesh Chaudhari, Nishad Tupe, Vishal Bhoyar \n",
    "###### Indiana University, Bloomington\n",
    "###### Applied Machine Learning SPR-19"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Abstract:\n",
    "Assessing risk when providing credit to an individual is a significant problem all lenders face. It is crucial to know how likely a person is to repay the borrowed money in an agreed upon time. There are many factors in a person’s life lenders can use to assess this risk such as previous credit history, occupation, age, location, credit card usage, and others. We will be studying these factors when trying to assess a loan application.  The dataset we will be using is provided from Home Credit which contains samples of over 600K credit application. We will use all the factors provided in the dataset to understand which factors are the most important in predicting a person’s default risk. We have done a baseline run on the provided data and calculated an accuracy score of 72.59% via the Kaggle service. Our goal is going to be to create a machine learning model that is well optimized and performs efficiently to asses this risk for leaders and provides them with the decision-making guidance they need to maximize their profits."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dataset Description\n",
    "We will use Kaggle Home Credit Default Risk Competition dataset for the study. The data is provided by Home Credit. \n",
    "The dataset contains seven different files. The relations between the data files are shown in ER diagram below.[1]\n",
    "* Application_train_test is main data file with information about each loan application at Home Credit.\n",
    "* Bureau file has applicant previous credits information from other financial institutions.  \n",
    "* Bureau_balance file provides monthly data about the previous credit from other financial institutions.  \n",
    "* Previous_application file has previous applications details for current applicant.   \n",
    "* POS_CASH_BALANCE file has monthly data about previous point of sale or cash loans \n",
    "* Credit_card_balance file has data for previous credit cards with Home Credit.   \n",
    "* Installments_payment provides applicant previous installments payment information.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "from IPython.display import Image\n",
    "![ERDigram.JPG](ERDigram.JPG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Research Design\n",
    "Understanding that this is a classification problem, we will use algorithms that are best suited for this study. Some of the algorithms we are considering are Logistic Regression, Random Forest, Support Vector Machines, and Gradient Boosted Trees. Logistic regression will be our baseline estimator as it is easy to understand and performs quickly. All algorithms will be tested with multiple hyperparameters during the grid search process. Due to the raw amount of data and one to many relationships present in them; we will be aggregating many of the datasets to an individual application.\n",
    "Along with this, we will focus on creating many different features via transformations in hopes that they will be good estimators for our dataset. We will be testing multiple different components during PCA analysis to lower the dimensionality of our data. Our goal will be to take these steps into pipelines so that we avoid any data leakages. Once we have run the models through the pipelines, we will focus on assessing their accuracy on a test and a validation set. The accuracy metrics we will focus on to help us choose an optimal model are; area under curve, accuracy score, log-loss, recall, precision, and F-score. We want to focus on assessing all these metrics together because there are many times that scores like accuracy might mislead us. In our dataset, due to an imbalanced distribution of a target class, we will be looking at all metrics together to give us a better grasp of the data and the validity of the models we have created.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Methods\n",
    "In this project, we will start exploring the relationships, trends, and anomalies using the EDA techniques. Exploratory analysis will also give us the opportunity to identify is missing data, what to do with it and perform necessary data formatting and preprocessing tasks which involves encoding the variable, imputing missing data and scaling features. Once all the data preparation steps are completed, we can build the pipelines and implement the baseline models, and then we tune the hyperparameters to make a slightly better model that beats our initial score.\n",
    "In the final stage, we report our findings based on the model's performance metrics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "from IPython.display import Image\n",
    "![Steps.png](Steps.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Project Timelines (Gantt Chart)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "from IPython.display import Image\n",
    "![Gantt.png](Gantt.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Work Contributions\n",
    "Vishal will focus on exploratory analysis and product description; Nishad will perform the data formatting, preprocessing and pipeline creation. Since the dataset contains many CSV files, all three of us will explore the feature engineering tasks. Currently, we have distributed responsibility based on relationships identified in CSV tables where each member of the team will scan two CSV files and explore features of interest.\n",
    "Once the EDA and data-preparation steps are completed, Naimesh will perform the model selection and hyperparameter tuning. \n",
    "We plan to sync as a team for each milestone of the project right from inception until we report our findings and performance metrics. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### References\n",
    "[1] https://www.kaggle.com/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
